#summary Testing the RDF Pipeline framework

= Introduction =

The RDF Pipeline framework has a regression test suite in RDF-Pipeline/t .  Since it runs as a mod_perl2 module in Apache2, the RDF Pipeline framework must be properly installed (as a mod_perl2 module) before the test suite can be run.  Once installed, the test suite may be run by issuing the following command from the RDF-Pipeline module directory:
{{{
make test
}}}

= Test suite conventions =
The test suite depends on two directories, specified in the following environment variables, which are normally set by set_env.sh:
  * *$RDF_PIPELINE_WWW_DIR* -- The Apache DOCUMENT_ROOT for the RDF Pipeline framework.
  * *$RDF_PIPELINE_MODULE_DIR* -- The RDF::Pipeline module directory, "RDF-Pipeline".

Each test is in a numbered directory RDF-Pipeline/t/0`*` and contains the following subdirectories or files.  All are optional unless noted:
  * *setup-files* -- This directory contains a snapshot of the $RDF_PIPELINE_WWW_DIR contents that should be used when running the test.  If it does not exist, the current $RDF_PIPELINE_WWW_DIR contents will be used.
  * *setup* -- A shell script to be run after the setup-files have been copied into $RDF_PIPELINE_WWW_DIR, but before the test is executed, to further set up the state of the machine, prior to running the test.
  * *test-script* -- (REQUIRED) The script that runs the test.  If it returns 0, then the test passes.  Otherwise, the test fails.  The default test-script generated by add-test.perl will run pipeline-request.perl to run the test and capture the results, sanitize-results to sanitize the results, and then "diff -r expected-files result-files" to determine whether the test passed or failed.  However, it can be modified to do whatever you need to do.
  * *result-files* -- If used by test-script, this directory will contain a snapshot of the $RDF_PIPELINE_WWW_DIR contents as it existed after running the test.  
  * *sanitize-results* -- A shell script to be run after the result-files have been captured, to further sanitize them (such as removing/normalizing dates or such) prior to comparing them with the expected-files.  
  * *expected-files* -- If used by test-script, this directory will contain a snapshot of the $RDF_PIPELINE_WWW_DIR contents as it _should_ be after successfully running the test.  If the result-files are the same as the expected-files (after running sanitize-results) then the test passes, otherwise it fails.

A test usually issues a series of HTTP requests against the Apache instance that is hosting the pipeline.  Hence, each test is normally driven by one or more URLs.  To facilitate regression testing, all information that is relevant to a test should be written to the $RDF_PIPELINE_WWW_DIR/test directory, as this is what run-one-test.perl expects.  By default this will include:
  * *apacheAccess.log* -- The Apache "access.log" file (excerpted and filtered through RDF-Pipeline/t/stripdates.perl to remove irrelevant detail)
  * *apacheError.log* -- The Apache "error.log" file (excerpted and filtered through RDF-Pipeline/t/filterlog.perl to remove irrelevant detail)
  * *testout* -- The concatenated results (filtered through RDF-Pipeline/t/stripdates.perl) of running 'curl -i' on the test URLs.  This includes both the headers and the content that were returned.


= Adding a new test =
The easiest way:
<ol>
<li>Configure a pipeline as needed for the test, so that the contents of $RDF_PIPELINE_WWW_DIR reflect the desired state prior to running the new test.  Do not place anything in the $RDF_PIPELINE_WWW_DIR/test directory, as this will be entirely cleared out prior to running the test.
</li>
<li>Run the command:
  {{{
  add-test URL
  }}}
  where URL is the URL to be requested from the pipeline.  This will run the test (by invoking the URL) and generate a new, numbered test directory containing:
    * setup-files
    * setup
    * test-script
    * result-files
    * sanitize-results
  However, at this point the new test directory has no expected-files, so the test will fail (as it should).
</li>
<li>If necessary, you may run the test as many times as you need (restarting Apache each time you have modified anything, of course), until you have convinced yourself that the result-files that have been produced are indeed correct for this test.  You can run the test by running:
    {{{
    run-one-test [nnnn]
    }}}
  where _nnnn_ is the directory for your new test.  (It defaults to the highest numbered test directory.)
</li>
<li>After you have verified that the result-files are correct, run
    {{{
    accept-test [nnnn]
    }}}
  to cause the current result-files to be saved as expected-files.
</li>
</ol>

= Deleting a test = 
Delete that particular subdirectory, e.g., 0003.

= Diagnosing a failed test =
To figure out why a test failed, cd into that test directory, and then do:
{{{
diff -r -x lm -x .svn expected-files result-files
}}}

= Updating the expected-files =
If you need to update the expected-files for a test (e.g., because a code change has caused the result-files to no longer match the expected-files), then *if you are certain that the current result-files are correct*, you can do
{{{
accept-test [nnnn]
}}}
to delete the existing expected-files and copy the current result-files to the expected-files.